# -*- coding: utf-8 -*-
"""QMLQcnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fro19yBVLXPUjh5790aMmkeodidjA_Vl
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# Load the NSL-KDD dataset
data = pd.read_csv('KDDTrain+.csv')

# Drop irrelevant columns
data = data.drop(columns=['service'])

# One-hot encode categorical features
enc = OneHotEncoder()
data = pd.concat([data, pd.DataFrame(enc.fit_transform(data[['protocol_type', 'flag']]).toarray(), columns=enc.get_feature_names())], axis=1)
data = data.drop(columns=['protocol_type', 'flag'])

# Scale numerical features
scaler = StandardScaler()
data[['duration', 'src_bytes', 'dst_bytes', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'num_compromised', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']] = scaler.fit_transform(data[['duration', 'src_bytes', 'dst_bytes', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'num_compromised', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']])

# Split the dataset into training, validation, and testing sets
train_data, test_data, train_labels, test_labels = train_test_split(data.iloc[:, :-1], data.iloc[:, -1], test_size=0.2, random_state=42)
train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)

import numpy as np
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten

# Load the preprocessed data
X = np.load('nsl_kdd/X.npy')
y = np.load('nsl_kdd/y.npy')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the architecture of the deep learning model
model = Sequential()
model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(64, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile the deep learning model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the deep learning model
model.fit(X_train, y_train, batch_size=128, epochs=10, validation_data=(X_test, y_test))

# Evaluate the performance of the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test loss: {loss}, Test accuracy: {accuracy}')

from qiskit import QuantumCircuit, QuantumRegister

# Assume that preprocessed_data is the pre-processed dataset that has been prepared
# according to the instructions in the previous steps.

# Define the number of qubits required to encode the data.
num_qubits = len(preprocessed_data[0])

# Initialize a quantum register with the required number of qubits.
qr = QuantumRegister(num_qubits)

# Create a quantum circuit with the initialized quantum register.
circ = QuantumCircuit(qr)

# Loop through the pre-processed data and apply amplitude encoding to each qubit.
for i, sample in enumerate(preprocessed_data):
    for j, value in enumerate(sample):
        circ.ry(2 * value, qr[j])

# Save the encoded quantum states to be used in further processing.
encoded_states = circ

from qiskit.aqua import QuantumInstance
from qiskit.aqua.algorithms import QSVM
from qiskit.aqua.components.feature_maps import SecondOrderExpansion
from qiskit.aqua.components.multiclass_extensions import AllPairs
from qiskit.aqua.utils import split_dataset_to_data_and_labels

# Assume that encoded_states is the encoded quantum states that have been prepared
# according to the instructions in the previous steps.

# Split the encoded states into training and test sets.
training_data, test_data, training_labels, test_labels = split_dataset_to_data_and_labels(encoded_states, labels)

# Define the number of qubits required to run the QSVM algorithm.
num_qubits = len(encoded_states.qubits)

# Initialize a feature map for the QSVM algorithm.
feature_map = SecondOrderExpansion(num_qubits)

# Initialize an all-pairs multiclass extension for the QSVM algorithm.
multiclass_extension = AllPairs(feature_map)

# Initialize a QSVM algorithm instance with the feature map and multiclass extension.
qsvm = QSVM(feature_map, training_data, training_labels, multiclass_extension)

# Initialize a quantum instance to run the QSVM algorithm on a quantum computing platform.
quantum_instance = QuantumInstance(backend, shots=num_shots)

# Run the QSVM algorithm on the quantum instance and obtain the results.
results = qsvm.run(quantum_instance)

# Save the results obtained from the quantum computing platform for further analysis and comparison with the classical deep learning results.

# Load test data
test_data = pd.read_csv('KDDTest+.txt', header=None, names=col_names)

# Preprocess test data
test_data = preprocess_data(test_data)

# Separate features and labels
test_labels = test_data.pop('label')
test_features = test_data.values

# Convert to one-hot encoding
test_features = encoder.transform(test_features).toarray()

# Normalize features
test_features = scaler.transform(test_features)

# Encode test features using quantum encoding
test_encoded = []
for features in test_features:
    encoded_features = quantum_encode(features, qubits)
    test_encoded.append(encoded_features)
test_encoded = np.array(test_encoded)

# Predict using quantum model
quantum_predictions = quantum_model.predict(test_encoded)

# Evaluate quantum model
quantum_accuracy = accuracy_score(test_labels, quantum_predictions)
quantum_precision = precision_score(test_labels, quantum_predictions, average='weighted')
quantum_recall = recall_score(test_labels, quantum_predictions, average='weighted')
quantum_f1 = f1_score(test_labels, quantum_predictions, average='weighted')

print(f'Quantum accuracy: {quantum_accuracy:.4f}')
print(f'Quantum precision: {quantum_precision:.4f}')
print(f'Quantum recall: {quantum_recall:.4f}')
print(f'Quantum F1 score: {quantum_f1:.4f}')

